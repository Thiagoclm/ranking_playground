{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thiagoclm/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/thiagoclm/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "def prepare_datasets():\n",
    "    \"\"\"\n",
    "    Load MSLR-WEB10K dataset using Hugging Face's `datasets` library.\n",
    "    Returns:\n",
    "        X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"philipphager/MSLR-WEB10k\")\n",
    "\n",
    "    def process_split(split):\n",
    "        \"\"\"\n",
    "        Processes a dataset split to extract features and labels.\n",
    "        Args:\n",
    "            split (Dataset): A split of the dataset (train, validation, test).\n",
    "        Returns:\n",
    "            X (ndarray): Feature matrix.\n",
    "            y (ndarray): Relevance scores.\n",
    "        \"\"\"\n",
    "        features = [np.array(f) for f in split[\"features\"]]  # Ensure features are NumPy arrays\n",
    "        relevance = np.array(split[\"labels\"], dtype=np.float32)  # Ensure labels are NumPy arrays\n",
    "\n",
    "        # Check for consistent feature dimensions\n",
    "        feature_length = len(features[0])\n",
    "        if not all(len(f) == feature_length for f in features):\n",
    "            raise ValueError(\"Inconsistent feature dimensions detected.\")\n",
    "\n",
    "        # Convert features list to a NumPy array\n",
    "        features = np.vstack(features)\n",
    "        return features, relevance\n",
    "\n",
    "    # Process each split\n",
    "    X_train, y_train = process_split(ds[\"train\"])\n",
    "    X_valid, y_valid = process_split(ds[\"validation\"])\n",
    "    X_test, y_test = process_split(ds[\"test\"])\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple neural network model for ranking\n",
    "class RankNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RankNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "# ListNet loss function\n",
    "def listnet_loss(y_pred, y_true):\n",
    "    y_pred_softmax = torch.softmax(y_pred, dim=0)\n",
    "    y_true_softmax = torch.softmax(y_true, dim=0)\n",
    "    return -torch.sum(y_true_softmax * torch.log(y_pred_softmax))\n",
    "\n",
    "# ListMLE loss function\n",
    "def listmle_loss(y_pred, y_true):\n",
    "    _, sorted_indices = torch.sort(y_true, descending=True)\n",
    "    y_pred_sorted = y_pred[sorted_indices]\n",
    "    return -torch.sum(torch.log(torch.softmax(y_pred_sorted, dim=0)))\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, loss_fn, X_train, y_train, X_valid, y_valid, lr=0.001, epochs=10, batch_size=128):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    y_valid_pred = model(torch.tensor(X_valid, dtype=torch.float32)).detach().numpy()\n",
    "    ndcg = ndcg_score([y_valid], [y_valid_pred])\n",
    "    print(f\"Validation NDCG: {ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6000,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Prepare datasets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     feature_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m136\u001b[39m  \u001b[38;5;66;03m# Number of features in MSLR-WEB10K\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     X_train, y_train, X_valid, y_valid, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Train ListNet\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining ListNet...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m, in \u001b[0;36mprepare_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features, relevance\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Process each split\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m X_valid, y_valid \u001b[38;5;241m=\u001b[39m process_split(ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     36\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m process_split(ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mprepare_datasets.<locals>.process_split\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mProcesses a dataset split to extract features and labels.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    y (ndarray): Relevance scores.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m features \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]]  \u001b[38;5;66;03m# Ensure features are NumPy arrays\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m relevance \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure labels are NumPy arrays\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Check for consistent feature dimensions\u001b[39;00m\n\u001b[1;32m     25\u001b[0m feature_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(features[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6000,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare datasets\n",
    "    feature_count = 136  # Number of features in MSLR-WEB10K\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_datasets()\n",
    "\n",
    "    # Train ListNet\n",
    "    print(\"Training ListNet...\")\n",
    "    listnet_model = RankNet(input_dim=feature_count)\n",
    "    train_model(listnet_model, listnet_loss, X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "    # Train ListMLE\n",
    "    print(\"Training ListMLE...\")\n",
    "    listmle_model = RankNet(input_dim=feature_count)\n",
    "    train_model(listmle_model, listmle_loss, X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    listnet_model.eval()\n",
    "    listmle_model.eval()\n",
    "    y_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_pred_listnet = listnet_model(y_test_tensor).detach().numpy()\n",
    "    y_pred_listmle = listmle_model(y_test_tensor).detach().numpy()\n",
    "\n",
    "    ndcg_listnet = ndcg_score([y_test], [y_pred_listnet])\n",
    "    ndcg_listmle = ndcg_score([y_test], [y_pred_listmle])\n",
    "\n",
    "    print(f\"Test NDCG (ListNet): {ndcg_listnet:.4f}\")\n",
    "    print(f\"Test NDCG (ListMLE): {ndcg_listmle:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
