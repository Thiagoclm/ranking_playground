{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are many types of sentences, all with different structures and complexities.\n",
      "Simply type the word into the sentence generator and we’ll do the rest.\n",
      "The train does not leave at morning.\n",
      "When someone gives a command (the imperative), they usually do not use a subject\n",
      "Actually, it is not easy to define a sentence. Grammarians do not all agree on what is or is not a sentence\n"
     ]
    }
   ],
   "source": [
    "docs = open('docs.txt').read()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are many types of sentences, all with different structures and complexities.', 'Simply type the word into the sentence generator and we’ll do the rest.', 'The train does not leave at morning.', 'When someone gives a command (the imperative), they usually do not use a subject', 'Actually, it is not easy to define a sentence. Grammarians do not all agree on what is or is not a sentence']\n",
      "Number of docs is 5\n"
     ]
    }
   ],
   "source": [
    "lines = docs.split('\\n')\n",
    "print(lines)\n",
    "print('Number of docs is', len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there are many types of sentences, all with different structures and complexities.', 'simply type the word into the sentence generator and we’ll do the rest.', 'the train does not leave at morning.', 'when someone gives a command (the imperative), they usually do not use a subject', 'actually, it is not easy to define a sentence. grammarians do not all agree on what is or is not a sentence']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of the corpus\n",
    "## 1. Lowercasing\n",
    "doc_prep_1 = [doc.lower() for doc in lines]\n",
    "print(doc_prep_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there are many types of sentences, all with different structures and complexities', 'simply type the word into the sentence generator and we’ll do the rest', 'the train does not leave at morning', 'when someone gives a command (the imperative), they usually do not use a subject', 'actually, it is not easy to define a sentence grammarians do not all agree on what is or is not a sentence']\n"
     ]
    }
   ],
   "source": [
    "## 2. remove punctuation\n",
    "doc_prep_2 = [doc.replace('.','') for doc in doc_prep_1]\n",
    "print(doc_prep_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['types sentences, different structures complexities', 'simply type word sentence generator we’ll rest', 'train does leave morning', 'when gives command (the imperative), they usually use subject', 'actually, easy to define sentence grammarians agree what sentence']\n"
     ]
    }
   ],
   "source": [
    "## 3. remove stopwords\n",
    "stopwords_list = open('english_stopwords.txt').read().split('\\n')\n",
    "doc_prep_3 = [' '.join([word for word in doc.split() if word not in stopwords_list]) for doc in doc_prep_2]\n",
    "print(doc_prep_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['types', 'sentences,', 'different', 'structures', 'complexities'], ['simply', 'type', 'word', 'sentence', 'generator', 'we’ll', 'rest'], ['train', 'does', 'leave', 'morning'], ['when', 'gives', 'command', '(the', 'imperative),', 'they', 'usually', 'use', 'subject'], ['actually,', 'easy', 'to', 'define', 'sentence', 'grammarians', 'agree', 'what', 'sentence']]\n"
     ]
    }
   ],
   "source": [
    "## Tokenization\n",
    "doc_tokens = [doc.split() for doc in doc_prep_3] \n",
    "print(doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['types', 'sentences,', 'different', 'structures', 'complexities', 'simply', 'type', 'word', 'sentence', 'generator', 'we’ll', 'rest', 'train', 'does', 'leave', 'morning', 'when', 'gives', 'command', '(the', 'imperative),', 'they', 'usually', 'use', 'subject', 'actually,', 'easy', 'to', 'define', 'sentence', 'grammarians', 'agree', 'what', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "# make doc_tokens in just one list\n",
    "all_tokens = [] \n",
    "for doc in doc_tokens:\n",
    "    all_tokens += doc\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inverted index from the corpus\n",
    "inverted_index = {}\n",
    "for token in all_tokens: # get the list of tokens\n",
    "    for i in range(len(lines)): #iterate over each document\n",
    "        if token in lines[i]: #check if the token is in the doc or if it was already considered\n",
    "            if token in inverted_index:\n",
    "                inverted_index[token].add(i+1) # if the token is already in the index, add the number of the doc\n",
    "            else:\n",
    "                inverted_index[token] = {i+1} # if the token is not in the index, create a new entry\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'types': {1},\n",
       " 'sentences,': {1},\n",
       " 'different': {1},\n",
       " 'structures': {1},\n",
       " 'complexities': {1},\n",
       " 'type': {1, 2},\n",
       " 'word': {2},\n",
       " 'sentence': {1, 2, 5},\n",
       " 'generator': {2},\n",
       " 'we’ll': {2},\n",
       " 'rest': {2},\n",
       " 'train': {3},\n",
       " 'does': {3},\n",
       " 'leave': {3},\n",
       " 'morning': {3},\n",
       " 'gives': {4},\n",
       " 'command': {4},\n",
       " '(the': {4},\n",
       " 'imperative),': {4},\n",
       " 'they': {4},\n",
       " 'usually': {4},\n",
       " 'use': {4},\n",
       " 'subject': {4},\n",
       " 'easy': {5},\n",
       " 'to': {2, 5},\n",
       " 'define': {5},\n",
       " 'agree': {5},\n",
       " 'what': {5}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
